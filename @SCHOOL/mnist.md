# MNIST 데이터셋을 활용한 MLPClassifier 구현 보고서

## 1. 데이터셋 구성

### 1.1 학습 및 테스트 데이터

-   **특성**: MNIST 데이터셋은 28x28 픽셀 크기의 흑백 이미지로 구성되어 있으며, 각 이미지는 손으로 쓴 숫자(0-9)를 나타냅니다. 이 데이터셋은 1개의 채널(흑백)을 가지고 있습니다.
-   **분할**: 학습 데이터(`trainset`)와 테스트 데이터(`testset`)는 PyTorch의 `torchvision.datasets.MNIST` 모듈을 통해 자동으로 분할됩니다. 데이터셋은 60,000개의 학습 이미지와 10,000개의 테스트 이미지로 나뉘며, 이는 대략 6:1의 비율입니다.

### 1.2 배치 처리

-   **DataLoader 설정**: `DataLoader` 객체를 사용하여 `trainloader`와 `testloader`를 생성할 때, `batch_size` 매개변수를 64로 설정했습니다. 이는 GPU를 활용하기 위한 표준적인 설정입니다. 배치 크기는 일반적으로 32나 64와 같이 설정됩니다. 저는 GPU를 사용하고 있기 때문에 큰 배치인 64 사용했습니다.

## 2. 학습 방법

### 2.1 모델 아키텍처

-   **실험용 모델**: 세 가지 MLP 모델을 비교했습니다:
    -   **MLP**: 은닉층 1개, 512노드.
    -   **MLP1Layer**: 은닉층 1개, 3200노드.
    -   **MLP10Layers**: 은닉층 10개, 각각 512노드.

### 2.2 노드의 개수 선정 이유

-   **하드웨어 최적화**: 노드의 개수를 32의 배수로 설정하는 것은 하드웨어 기반의 최적화를 위함입니다. 신경망과 딥러닝은 행렬 연산을 많이 사용하는데, 이는 부동소수점 연산(FLOPs)의 주요 원천입니다. CPU는 단일 명령 여러 데이터(SIMD) 연산을 2의 거듭제곱 크기의 배치로 실행합니다.
    GPU의 경우에도 마찬가지입니다. CUDA Runtime API를 통해 할당된 메모리는 최소 256바이트로 정렬되도록 보장됩니다. 따라서 warp 크기의 배수인 스레드 블록 크기를 선택하면 올바르게 정렬된 warp에 의한 메모리 액세스를 용이하게 합니다.
    즉, GPU를 사용할 때 32의 배수는 메모리 액세스를 최적화하고 처리 속도를 향상시킬 수 있습니다.

### 2.3 최적화 알고리즘

-   **SGD 사용**: SGD(Stochastic Gradient Descent)를 사용했습니다. 이는 효율성, 빠른 수렴 속도, 좋은 일반화 능력 및 지역 최소값 탈출 능력 때문에 선택되었습니다.

### 2.4 정규화 및 GPU 활용

-   **정규화**: 데이터에 `transforms.Normalize((0.5,), (0.5,))`를 적용하여 정확도를 향상시켰습니다.
-   **GPU 활용**: 학습 속도를 높이기 위해 PyTorch와 GPU를 사용했습니다.

## 3. 실험 결과 및 분석

### 3.1 실험 결과

-   **손실 값 비교**: 2번의 에포크 후, MLP의 loss 값은 0.40, MLP1Layer의 loss 값은 0.16, MLP10Layers의 loss 값은 2.29로 나타났습니다. 이를 바탕으로 MLP1Layer가 가장 효과적인 모델로 선정되었습니다.

### 3.2 학습률 설정

-   **학습률 0.01**: 학습률은 대부분의 딥러닝 작업에 적합한 0.01로 설정했습니다. 이 값은 널리 사용되며, 학습 과정이 잘 진행되어 추가 조정이 필요하지 않았습니다.

## 4. 최종 결과 평가

### 4.1 성능 지표

-   **정확도**: 정확도

는 인간에게 가장 직관적인 평가 지표입니다. 모델은 테스트 데이터셋에서 95.35%의 정확도를 달성했습니다.

-   **F1 지표**: F1 지표는 정밀도와 재현율의 조화 평균을 나타냅니다. 이는 클래스 불균형이 있는 경우 또는 두 지표 모두 중요한 경우에 유용합니다.  
    결과 : 0.95301

### 4.2 인식률 결과 및 분석

-   **오분류 사례 분석**: 오분류된 사례들 중 다수는 숫자의 모양이 많이 이상한 경우였습니다. 이는 모델이 표준적이지 않은 글씨체나 모호한 형태의 숫자를 인식하는데 어려움을 겪을 수 있음을 시사합니다.

### 4.3 향후 개선 방향

-   **모델 복잡성 증가**: MLP의 레이어를 추가하거나 노드 수를 조정하여 모델의 복잡성을 증가시킬 수 있습니다.
-   **데이터 증강**: 모호한 형태의 숫자에 대한 모델의 인식 능력을 개선하기 위해 데이터 증강 기법을 적용할 수 있습니다.
-   **하이퍼파라미터 조정**: 학습률, 에포크 수 등의 하이퍼파라미터를 조정하여 모델의 성능을 최적화할 수 있습니다.
-   **CNN 사용**: 인식률을 향상시키기 위해 Convolutional Neural Network(CNN)을 사용하는 것도 고려할 수 있습니다. CNN은 이미지 처리에 특화된 구조로, 복잡한 패턴과 특징을 더 잘 인식할 수 있습니다.
